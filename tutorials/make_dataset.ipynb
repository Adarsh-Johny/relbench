{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "098e9452-03b7-4d33-acf2-72d507f3f96d",
   "metadata": {},
   "source": [
    "You can work with your own data in the RelBench framework. This tutorial walks you through the steps to create custom datasets and tasks. Code in this notebook has been adapted from `relbench/datasets/f1.py` and `relbench/tasks/f1.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e840a5f-a6e4-4aff-b6b8-4a777dc2045a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T22:51:35.258604Z",
     "iopub.status.busy": "2024-07-22T22:51:35.258167Z",
     "iopub.status.idle": "2024-07-22T22:51:35.269009Z",
     "shell.execute_reply": "2024-07-22T22:51:35.268587Z",
     "shell.execute_reply.started": "2024-07-22T22:51:35.258581Z"
    }
   },
   "source": [
    "# Custom Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd3d3e7-e810-483e-beeb-e74c66c8ad48",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e78e8039-1bb8-4265-ac9e-65520deab3a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T23:52:16.889805Z",
     "iopub.status.busy": "2024-07-22T23:52:16.889343Z",
     "iopub.status.idle": "2024-07-22T23:52:16.920111Z",
     "shell.execute_reply": "2024-07-22T23:52:16.919024Z",
     "shell.execute_reply.started": "2024-07-22T23:52:16.889765Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pooch\n",
    "\n",
    "from relbench.base import Database, Dataset, Table\n",
    "from relbench.utils import unzip_processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff46a3c-a202-4f27-8533-1dfdc6d41dbf",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10240d58-21f8-42ba-a936-031114aec2fc",
   "metadata": {},
   "source": [
    "To define a custom dataset, we subclass the `relbench.base.Dataset` class. This requires specifying 3 things:\n",
    "1. `val_timestamp` of type `pd.Timestamp`\n",
    "2. `test_timestamp` of type `pd.Timestamp`\n",
    "3. a `make_db` function which returns a `relbench.base.Database` object\n",
    "\n",
    "These are described in further detail below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee17a117-c8f0-4f43-9f9d-7b41c0886acb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T23:11:18.430211Z",
     "iopub.status.busy": "2024-07-22T23:11:18.429861Z",
     "iopub.status.idle": "2024-07-22T23:11:18.446220Z",
     "shell.execute_reply": "2024-07-22T23:11:18.445784Z",
     "shell.execute_reply.started": "2024-07-22T23:11:18.430194Z"
    }
   },
   "source": [
    "### Temporal splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7deac8d1-bdf1-4f96-9cb2-93cd665ca71d",
   "metadata": {},
   "source": [
    "`val_timestamp` and `test_timestamp` define a unified temporal splitting for the dataset. All tasks defined on the dataset will provide train/val/test sets based on this splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e29e72-420f-481b-a986-e2369fc3c88a",
   "metadata": {},
   "source": [
    "Only database rows upto `val_timestamp` should be used by the model to predict over the val set. Similarly, only database rows upto `test_timestamp` should be used by the model to predict over the test set. Rows after `test_timestamp` are only used for computing ground truth labels for the test set. Importantly, only rows upto `val_timestamp` can be used to obtain ground truth labels for the train set; and only rows upto `test_timestamp` can be used to obtain ground truth labels for the val set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533a2345-bf6f-4922-8454-446af46b6e3f",
   "metadata": {},
   "source": [
    "This is important to prevent temporal leakage of information, which we will see is an important consideration in many aspects of Relational Deep Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0714ecc7-7eb6-41da-a1ce-8ddfa868ac07",
   "metadata": {},
   "source": [
    "### The make_db function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47dcff3-60ab-43b7-80c3-a61e45493b40",
   "metadata": {},
   "source": [
    "The raw data for your dataset can be in any form. You will need to preprocess your dataset into the RelBench format to work with it in RelBench. The `make_db` function is the place to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddb7d3e-55e0-49f2-ac41-43f9454b4f3e",
   "metadata": {},
   "source": [
    "Inside the `make_db` function we first download the raw files (or read from the local filesystem) and then create a `relbench.base.Database` object out of those. Thus, the `make_db` functions serves as documentation for your pre-processing steps, while also conveniently allowing you to develop and debug them within the RelBench framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec0b02e-ce3e-426b-8fe2-38b8fa8d20c8",
   "metadata": {},
   "source": [
    "### The Database object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241e9f15-5dd4-4f31-be2f-28382aa8cf4b",
   "metadata": {},
   "source": [
    "The `relbench.base.Database` object is simply a collection of named `relbench.base.Table` objects. A `relbench.base.Table` object is instantiated by providing:\n",
    "1. `df` of type `pd.DataFrame` representing the table content.\n",
    "2. `fkey_col_to_pkey_table` dict to map foreign-key columns in this table to their primary-key table names.\n",
    "3. `pkey_col`, the name of the primary key column, if any.\n",
    "4. `time_col`, the name of the time column, if available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53711365-a256-4d78-a8f4-1b06ac0d7600",
   "metadata": {},
   "source": [
    "The `time_col` denotes the creation time of a row in the database. If absent or `None`, the row is treated as if it was created at time `-inf`. Note that there can be other columns of type `pd.Timestamp` which are not necessarily the creation time. For example, there can be a `CreationDate` column in a `Users` table which would be suitable for `time_col`, but also a `DateOfBirth` column, which is better treated as an ordinary feature column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d382e5d-8e33-45b0-a676-7b2fe9e2f1cd",
   "metadata": {},
   "source": [
    "## Annotated Sample Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "165d2698-ae4c-44f9-be75-24cf55028bde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T00:25:06.399354Z",
     "iopub.status.busy": "2024-07-23T00:25:06.398839Z",
     "iopub.status.idle": "2024-07-23T00:25:06.455150Z",
     "shell.execute_reply": "2024-07-23T00:25:06.454318Z",
     "shell.execute_reply.started": "2024-07-23T00:25:06.399315Z"
    }
   },
   "outputs": [],
   "source": [
    "class F1Dataset(Dataset):\n",
    "    ################################################################################\n",
    "    # Choose the val_timestamp and test_timestamp carefully\n",
    "    ################################################################################\n",
    "    val_timestamp = pd.Timestamp(\"2005-01-01\")\n",
    "    test_timestamp = pd.Timestamp(\"2010-01-01\")\n",
    "\n",
    "    def make_db(self) -> Database:\n",
    "        r\"\"\"Process the raw files into a database.\"\"\"\n",
    "        ################################################################################\n",
    "        # The raw files are at this URL. You can use any URL or even local files.\n",
    "        ################################################################################\n",
    "        url = \"https://relbench.stanford.edu/data/relbench-f1-raw.zip\"\n",
    "\n",
    "        path = pooch.retrieve(\n",
    "            url,\n",
    "            known_hash=\"2933348953b30aa9723b4831fea8071b336b74977bbcf1fb059da63a04f06eba\",\n",
    "            progressbar=True,\n",
    "            processor=unzip_processor,\n",
    "        )\n",
    "\n",
    "        path = os.path.join(path, \"raw\")\n",
    "\n",
    "        ################################################################################\n",
    "        # Here, we read from the raw CSV files\n",
    "        ################################################################################\n",
    "        circuits = pd.read_csv(os.path.join(path, \"circuits.csv\"))\n",
    "        drivers = pd.read_csv(os.path.join(path, \"drivers.csv\"))\n",
    "        results = pd.read_csv(os.path.join(path, \"results.csv\"))\n",
    "        races = pd.read_csv(os.path.join(path, \"races.csv\"))\n",
    "        standings = pd.read_csv(os.path.join(path, \"driver_standings.csv\"))\n",
    "        constructors = pd.read_csv(os.path.join(path, \"constructors.csv\"))\n",
    "        constructor_results = pd.read_csv(os.path.join(path, \"constructor_results.csv\"))\n",
    "        constructor_standings = pd.read_csv(\n",
    "            os.path.join(path, \"constructor_standings.csv\")\n",
    "        )\n",
    "        qualifying = pd.read_csv(os.path.join(path, \"qualifying.csv\"))\n",
    "\n",
    "        ################################################################################\n",
    "        # It is important to understand the data.\n",
    "        # This can point to columns which should be removed.\n",
    "        # The most important of these is temporal leakage columns, which we will\n",
    "        # discuss in detail later.\n",
    "        ################################################################################\n",
    "\n",
    "        # Remove columns that are irrelevant, leak time,\n",
    "        # or have too many missing values\n",
    "\n",
    "        # Drop the Wikipedia URL and some time columns with many missing values\n",
    "        races.drop(\n",
    "            columns=[\n",
    "                \"url\",\n",
    "                \"fp1_date\",\n",
    "                \"fp1_time\",\n",
    "                \"fp2_date\",\n",
    "                \"fp2_time\",\n",
    "                \"fp3_date\",\n",
    "                \"fp3_time\",\n",
    "                \"quali_date\",\n",
    "                \"quali_time\",\n",
    "                \"sprint_date\",\n",
    "                \"sprint_time\",\n",
    "            ],\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        # Drop the Wikipedia URL as it is unique for each row\n",
    "        circuits.drop(\n",
    "            columns=[\"url\"],\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        # Drop the Wikipedia URL (unique) and number (803 / 857 are nulls)\n",
    "        drivers.drop(\n",
    "            columns=[\"number\", \"url\"],\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        # Drop the positionText, time, fastestLapTime and fastestLapSpeed\n",
    "        results.drop(\n",
    "            columns=[\n",
    "                \"positionText\",\n",
    "                \"time\",\n",
    "                \"fastestLapTime\",\n",
    "                \"fastestLapSpeed\",\n",
    "            ],\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        # Drop the positionText\n",
    "        standings.drop(\n",
    "            columns=[\"positionText\"],\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        # Drop the Wikipedia URL\n",
    "        constructors.drop(\n",
    "            columns=[\"url\"],\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        # Drop the positionText\n",
    "        constructor_standings.drop(\n",
    "            columns=[\"positionText\"],\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        # Drop the status as it only contains two categories, and\n",
    "        # only 17 rows have value 'D' (0.138%)\n",
    "        constructor_results.drop(\n",
    "            columns=[\"status\"],\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        # Drop the time in qualifying 1, 2, and 3\n",
    "        qualifying.drop(\n",
    "            columns=[\"q1\", \"q2\", \"q3\"],\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        ################################################################################\n",
    "        # Make sure to properly process time columns into pd.Timestamp datatype.\n",
    "        # Sometimes, you might need to handle timezone information carefully to do\n",
    "        # this correctly.\n",
    "        # If time information can be inferred for other tables, it might help to add\n",
    "        # as it makes temporal sampling more effective.\n",
    "        ################################################################################\n",
    "\n",
    "        # replase missing data and combine date and time columns\n",
    "        races[\"time\"] = races[\"time\"].replace(r\"^\\\\N$\", \"00:00:00\", regex=True)\n",
    "        races[\"date\"] = races[\"date\"] + \" \" + races[\"time\"]\n",
    "        # Convert date column to pd.Timestamp\n",
    "        races[\"date\"] = pd.to_datetime(races[\"date\"])\n",
    "\n",
    "        # add time column to other tables\n",
    "        results = results.merge(races[[\"raceId\", \"date\"]], on=\"raceId\", how=\"left\")\n",
    "        standings = standings.merge(races[[\"raceId\", \"date\"]], on=\"raceId\", how=\"left\")\n",
    "        constructor_results = constructor_results.merge(\n",
    "            races[[\"raceId\", \"date\"]], on=\"raceId\", how=\"left\"\n",
    "        )\n",
    "        constructor_standings = constructor_standings.merge(\n",
    "            races[[\"raceId\", \"date\"]], on=\"raceId\", how=\"left\"\n",
    "        )\n",
    "\n",
    "        qualifying = qualifying.merge(\n",
    "            races[[\"raceId\", \"date\"]], on=\"raceId\", how=\"left\"\n",
    "        )\n",
    "\n",
    "        # Subtract a day from the date to account for the fact\n",
    "        # that the qualifying time is the day before the main race\n",
    "        qualifying[\"date\"] = qualifying[\"date\"] - pd.Timedelta(days=1)\n",
    "\n",
    "        ################################################################################\n",
    "        # Make sure that the missing data has been parsed properly.\n",
    "        # Following Pandas, we represent missing values with NaNs in the dataframe.\n",
    "        ################################################################################\n",
    "\n",
    "        # Replace \"\\N\" with NaN in results tables\n",
    "        results = results.replace(r\"^\\\\N$\", np.nan, regex=True)\n",
    "\n",
    "        # Replace \"\\N\" with NaN in circuits tables, especially\n",
    "        # for the column `alt` which has 3 rows of \"\\N\"\n",
    "        circuits = circuits.replace(r\"^\\\\N$\", np.nan, regex=True)\n",
    "        # Convert alt from string to float\n",
    "        circuits[\"alt\"] = circuits[\"alt\"].astype(float)\n",
    "\n",
    "        # Convert non-numeric values to NaN in the specified column\n",
    "        results[\"rank\"] = pd.to_numeric(results[\"rank\"], errors=\"coerce\")\n",
    "        results[\"number\"] = pd.to_numeric(results[\"number\"], errors=\"coerce\")\n",
    "        results[\"grid\"] = pd.to_numeric(results[\"grid\"], errors=\"coerce\")\n",
    "        results[\"position\"] = pd.to_numeric(results[\"position\"], errors=\"coerce\")\n",
    "        results[\"points\"] = pd.to_numeric(results[\"points\"], errors=\"coerce\")\n",
    "        results[\"laps\"] = pd.to_numeric(results[\"laps\"], errors=\"coerce\")\n",
    "        results[\"milliseconds\"] = pd.to_numeric(\n",
    "            results[\"milliseconds\"], errors=\"coerce\"\n",
    "        )\n",
    "        results[\"fastestLap\"] = pd.to_numeric(results[\"fastestLap\"], errors=\"coerce\")\n",
    "\n",
    "        # Convert drivers date of birth to datetime\n",
    "        drivers[\"dob\"] = pd.to_datetime(drivers[\"dob\"])\n",
    "\n",
    "        ################################################################################\n",
    "        # Here, we collect all tables in the database as relbench.base.Table objects.\n",
    "        ################################################################################\n",
    "\n",
    "        tables = {}\n",
    "\n",
    "        tables[\"races\"] = Table(\n",
    "            df=pd.DataFrame(races),\n",
    "            fkey_col_to_pkey_table={\n",
    "                \"circuitId\": \"circuits\",\n",
    "            },\n",
    "            pkey_col=\"raceId\",\n",
    "            time_col=\"date\",\n",
    "        )\n",
    "\n",
    "        tables[\"circuits\"] = Table(\n",
    "            df=pd.DataFrame(circuits),\n",
    "            fkey_col_to_pkey_table={},\n",
    "            pkey_col=\"circuitId\",\n",
    "            time_col=None,\n",
    "        )\n",
    "\n",
    "        tables[\"drivers\"] = Table(\n",
    "            df=pd.DataFrame(drivers),\n",
    "            fkey_col_to_pkey_table={},\n",
    "            pkey_col=\"driverId\",\n",
    "            time_col=None,\n",
    "        )\n",
    "\n",
    "        tables[\"results\"] = Table(\n",
    "            df=pd.DataFrame(results),\n",
    "            fkey_col_to_pkey_table={\n",
    "                \"raceId\": \"races\",\n",
    "                \"driverId\": \"drivers\",\n",
    "                \"constructorId\": \"constructors\",\n",
    "            },\n",
    "            pkey_col=\"resultId\",\n",
    "            time_col=\"date\",\n",
    "        )\n",
    "\n",
    "        tables[\"standings\"] = Table(\n",
    "            df=pd.DataFrame(standings),\n",
    "            fkey_col_to_pkey_table={\"raceId\": \"races\", \"driverId\": \"drivers\"},\n",
    "            pkey_col=\"driverStandingsId\",\n",
    "            time_col=\"date\",\n",
    "        )\n",
    "\n",
    "        tables[\"constructors\"] = Table(\n",
    "            df=pd.DataFrame(constructors),\n",
    "            fkey_col_to_pkey_table={},\n",
    "            pkey_col=\"constructorId\",\n",
    "            time_col=None,\n",
    "        )\n",
    "\n",
    "        tables[\"constructor_results\"] = Table(\n",
    "            df=pd.DataFrame(constructor_results),\n",
    "            fkey_col_to_pkey_table={\"raceId\": \"races\", \"constructorId\": \"constructors\"},\n",
    "            pkey_col=\"constructorResultsId\",\n",
    "            time_col=\"date\",\n",
    "        )\n",
    "\n",
    "        tables[\"constructor_standings\"] = Table(\n",
    "            df=pd.DataFrame(constructor_standings),\n",
    "            fkey_col_to_pkey_table={\"raceId\": \"races\", \"constructorId\": \"constructors\"},\n",
    "            pkey_col=\"constructorStandingsId\",\n",
    "            time_col=\"date\",\n",
    "        )\n",
    "\n",
    "        tables[\"qualifying\"] = Table(\n",
    "            df=pd.DataFrame(qualifying),\n",
    "            fkey_col_to_pkey_table={\n",
    "                \"raceId\": \"races\",\n",
    "                \"driverId\": \"drivers\",\n",
    "                \"constructorId\": \"constructors\",\n",
    "            },\n",
    "            pkey_col=\"qualifyId\",\n",
    "            time_col=\"date\",\n",
    "        )\n",
    "\n",
    "        return Database(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346ad85f-36df-4dee-ad9b-3c3e389f5570",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
